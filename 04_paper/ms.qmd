---
title: "Creation and validation of the LEVANTE core tasks: Internationalized measures of learning and development for children ages 5-12 years"
shorttitle: "LEVANTE Measures"

author: 
  - name: "George Kachergis*"
    affiliations:
      - ref: stanford
  - name: "Fionnuala O'Reilly*"
    affiliations:
      - ref: stanford
  - name: "Mika Braginsky"
    orcid: 0000-0001-9039-3220
    affiliations:
      - ref: stanford
  - name: "Xingyao Xiao"
    affiliations:
      - ref: stanford
  - name: "Amy A. Lightbody"
    affiliations:
      - ref: stanford
  - name: "Katherine Adams Shannon"
    affiliations:
      - ref: stanford
  - name: "Zachary Watson"
    affiliations:
      - ref: stanford
  - name: "Lijin Zhang"
    affiliations:
      - ref: stanford
  - name: "Rebecca Zhu"
    affiliations:
      - ref: stanford
  - name: "Adani Abutto"
    affiliations:
      - ref: stanford
  - name: "Wanjing Anya Ma"
    affiliations:
      - ref: stanford
  - name: "Bria Long"
    affiliations:
      - ref: ucsd
  - name: "Tonya Murray"
    affiliations:
      - ref: stanford
  - name: "Jason Yeatman"
    affiliations:
      - ref: stanford
  - name: "Michael Sulik"
    affiliations:
      - ref: stanford
  - name: "Jelena Obradović"
    affiliations:
      - ref: stanford
  - name: "Nichola Jenkins"
    affiliations:
      - ref: western
  - name: "Daniel Ansari"
    affiliations:
      - ref: western
  - name: "Maria Camilla Perfetti"
    affiliations:
      - ref: uniandes
  - name: "Julian Mariño"
    affiliations:
      - ref: uniandes
  - name: "Luise Hornoff"
    affiliations:
      - ref: mpieva
  - name: "Manuel Bohn"
    affiliations:
      - ref: mpieva 
      - ref: leuphana
  - name: "Daniel Haun"
    affiliations:
      - ref: mpieva
  - name: "Nilam Ram"
    affiliations:
      - ref: stanford
  - name: "Benjamin W. Domingue"
    affiliations:
      - ref: stanford
  - name: "Michael C. Frank"
    orcid: 0000-0002-7551-4378
    affiliations:
      - ref: stanford
    corresponding: true
    email: mcfrank@stanford.edu
    # role:
    #   - conceptualization
    #   - writing
    #   - editing
    #   - supervision

affiliations:
  - id: stanford
    name: "Stanford University"
  - id: ucsd
    name: "University of California, San Diego"
  - id: western
    name: "Western University"
  - id: uniandes
    name: "Universidad de los Andes"
  - id: mpieva
    name: "Max Planck Institute for Evolutionary Anthropology"
  - id: leuphana
    name: "Leuphana University"

author-note:
  disclosures:
    financial-support: "This work was supported by the Jacobs Foundation."
    # conflict-of-interest: The author has no conflict of interest to declare.

abstract: We present the Learning Variability Network Exchange (LEVANTE) core tasks, a set of nine short and engaging computer adaptive tasks designed to assess learning and development in children ages 5--12 years across a wide range of languages and cultures. Using a simple and uniform multi-alternative forced-choice format, these tasks measure constructs including math, executive function, language, reasoning, and social cognition and can be administered on a tablet or computer both in person and remotely, with all materials openly available. We describe the design and selection of these tasks, and then report on their reliability and validity in a sample of 1034 children recruited from sites in Colombia, Germany, and Canada. Tasks are scored using multi-group item response theory models, allowing testing for measurement invariance. The parameters of these models can then be used to create computer adaptive versions of the tasks, allowing the entire battery to be given in around an hour. We discuss the use,  ongoing refinement, and extension of these tasks in the service of creating an open dataset to describe variability in children's development and learning across contexts.

keywords: ["cognitive development"]
word-count: true

bibliography: library.bib

floatsintext: true
numbered-lines: true
# draft: false
mask: false

# appendix
number-sections: true
appendix-style: default


# figurelist: no
# tablelist: no
# footnotelist: no

format:
  apaquarto-pdf:
    documentmode: man
    keep-tex: true
    include-in-header: preamble.tex
    fig-format: png
    # fig-pos: H        
    # tbl-pos: H  

knitr:
  opts_chunk:
    ft.arraystretch: 1.25

execute: 
  eval: false
  echo: false
  message: false
  warning: false
  error: true
  cache: true
---

```{r setup}
#| include: false
#| cache: false

library(tidyverse)
library(here)
library(glue)
library(purrr)
library(viridis)
library(flextable)
library(lavaan)
library(mirt)
library(broom.mixed)
library(lmerTest)
library(png)
library(emmeans)

source(here("03_summaries","plotting_helper.R"))
source(here("plot_settings.R"))
```

```{r settings}
#| cache: false

site_labels <- c("uniandes-co" = "pilot_uniandes_co",
                 "mpieva-de" = "pilot_mpieva_de",
                 "western-ca" = "pilot_western_ca")
core_tasks   <- c("matrix","mrot","math","hf","mg","sds",
                  "trog","vocab","tom")

sites <- names(site_labels)
site_pal <- solarized_pal()(length(sites)) |> rlang::set_names(sites)

set_flextable_defaults(theme_fun = theme_apa, arraystretch = 1, padding = 1)

# seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)

# helper for formatting p-values
pval_fmt <- function(x) ifelse(x < .001, "<.001", sub("^0\\.", ".", sprintf("%.3f", x)))
```

# Introduction

Developmental variability and change during childhood is a focus of intense theoretical and practical interest. From tracking children's growth over time to evaluating intervention outcomes and exploring environmental and contextual moderators, a wide range of scientific goals require accurate assessments. Ideal psychological assessments provide efficient, reliable, and valid measures of particular constructs that can be applied across a range of ages, situations, and contexts. However, for most researchers searching for measures, the reality falls far short of this ideal. This gap is driven by obstacles such as limited coverage across age groups, variability across contexts, and restricted accessibility of instruments.

Because children's overall capacities are so dependent on their age, it can be very challenging to use the same measure across children of different ages. Young children require simple tasks that are not verbally demanding, while older children can answer more complicated questions, and can become bored if given too many simple questions. In addition, younger children typically require shorter tasks, often reducing measurement reliability. Yet giving different tasks to different ages can mean that scores are not comparable to one another, making tracking developmental growth challenging in many domains.  

A second set of challenges concern cross-context comparisons. Ideal developmental measures should be validated in a global context and applicable to children across many cultures and languages. Child development is an issue of global importance [@grantham2007developmental;@masten2014global;@aboud2015global], yet the vast majority of developmental measures are developed in very specific (often English-speaking) contexts [@kidd2022diverse;@singh2023diversity;@richter2019early]. 

A final set of challenges has to do with accessibility. Many gold-standard measures are commercially distributed. They are costly for researchers to use, and in addition, publishers may place barriers on new translation and adaptation. Publishers also typically hold both item information and normative data closely, blocking many types of secondary investigation. 

## LEVANTE

The Learning Variability Network Exchange (LEVANTE) was designed to address these challenges [@frank2025] by validating an openly accessible set of measures applicable to a wide developmental age range within varied contexts. LEVANTE provides a software framework for data collection with these measures: researchers can use the LEVANTE dashboard, a web interface to assign both surveys and task bundles to groups of children, caregivers, and teachers. Data collected via the LEVANTE dashboard are harmonized and validated and become accessible through a data repository, first to the researchers who collect them and then, through regular data releases, to the broader research community. Through a partnership with the Jacobs Foundation, sites around the world are funded to collect longitudinal data from children using the LEVANTE framework. The eventual goal of LEVANTE is to create and openly share a rich dataset and suite of measures documenting children’s learning and development across contexts.

The current manuscript introduces the LEVANTE core tasks, a suite of behavioral measures for children. In our initial development of the task battery, we cast a broad net for important constructs in child development with well-accepted measures that had been used internationally. This process is described in @frank2025. The broad constructs that we selected were executive function, language, mathematics, reasoning, and social cognition, with these being instantiated through a number of well-accepted tasks. 

To create our core tasks, we selected pre-existing measures from the literature that tapped each of these constructs. When possible, we prioritized measures with strong psychometric properties, previous use across a broad range of cultures, applicability across a broad range of ages, and lack of commercial or licensing constraints. Although very few measures met all our criteria, this search yielded a series of candidate measures, which we adapted and re-implemented in an open source web platform. @tbl-tasks shows these tasks, organized by construct. Some of these implementations remained quite close to prior versions, while others by necessity required the creation of additional test items or changes to specifics of their procedure. 

In addition to the constructs described above, we were also interested in the assessment of literacy. The LEVANTE core tasks battery makes use of a number of previously-validated literacy tasks from the Rapid Online Assessment of Reading [@yeatman2021], including Single Word Reading (ROAR-Word) [@yeatman2021rapid; @ma2025roar], Sentence Reading Efficiency (ROAR-Sentence) [@yeatman2024development], and Phonological Awareness (ROAR-Phoneme) [@gijbels2024rapid]. Because they are extensively described elsewhere, we do not report on these tasks here, though we make use of literacy data for validation of language measures. For validation of our executive function measures, we also included a commercially-available broad measure of executive function, the Minnesota Executive Function Scale (MEFS) [@carlson2021]. 

```{r}
#| label: tbl-tasks
#| tbl-cap: "The LEVANTE core tasks, presented with their internal label as well as prior labels used in the literature."

gray_border <- officer::fp_border(color = "gray")
task_table <- tribble(
  ~"Construct"         , ~"LEVANTE task name"     , ~"Prior/Source task name",
   "Executive Function",  "Hearts and Flowers"    ,  "Hearts and Flowers",
   ""                  ,  "Memory"                ,  "AMES Memory Game",
   ""                  ,  "Same & Different"      ,  "The Same Different Selection Task",
   "Language"          ,  "Vocabulary"            ,  "Picture Vocabulary",
   ""                  ,  "Sentence Understanding",  "Test for Reception of Grammar (TROG)",
   "Math"              ,  "Math"                  ,  "Early Grades Math Assessment (EGMA)",
   "Reasoning"         ,  "Pattern Matching"      ,  "Matrix Reasoning",
   ""                  ,  "Shape Rotation"        ,  "Mental Rotation",
   "Social Cognition"  ,  "Stories"               ,  "Theory of Mind"
)

line_rows <- task_table |> mutate(i = 1:n() - 1) |> filter(Construct != "") |> slice(-1) |> pull(i)
task_table |>
  flextable() |>
  align_text_col() |>
  hline(i = line_rows, border = gray_border) |>
  autofit()
```

## The current study

Here we report on the development and validation of the LEVANTE core tasks. This was an iterative process in which data from 5--12 year old children were across three sites: Bogota, Colombia (uniandes-co); Leipzig, Germany (mpieva-de); and Ottawa, Canada (western-ca).^[In the remainder of the manuscript, we refer to these sites by their site designators, uniandes-co, mpieva-de, and western-ca. These designators mark the research group and country of data collection. We make this choice to avoid the inference that these data are particularly representative of Canadian, German, or Colombian children more broadly.] In some cases, we used these data during the data collection process to make minor changes to the tasks. We also used data from these three pilot sites both to provide initial evidence on the reliability and validity of the measures and to develop efficient, computer adaptive task (CAT) versions of many of the tasks. 

Psychometric models based on item-response theory (IRT) are a key component of our process for developing the LEVANTE core tasks [@embretson2013]. IRT provides a family of models that allow the joint estimation of the difficulty of individual task items (e.g., math questions) and the ability of individual children. A fitted IRT model provides task parameters that can be used to estimate the ability of a new test taker given their responses on some or all of the same items. In addition, IRT parameters are used in the construction of CATs, which choose the most relevant items to give to estimate the ability of a particular individual. Critically for our purposes, the use of IRT models means that we can provide comparable scores on the same scale to a younger child who saw mostly easier task items and an older child who saw harder items; these models thus allow us to address our first key challenge posed (limited coverage across age groups) above.  

Because our data come from three sites, each with their own translations and adaptations of the specific tasks, we can also use multi-group IRT models to explore the question of invariance: whether measures function similarly across different groups [@putnick2016]. While measurement invariance is more commonly discussed in the factor analytic literature, it is also applicable to IRT (where it is sometimes analyzed at the level of individual test items as "differential item function" across groups) [@thissen2025]. Here we use multi-group model comparisons (described below) to investigate whether our tasks measure similarly structured constructs across groups. In particular, where possible, we aim for *scalar invariance*, in which individuals from different groups still show the same relative ordering of difficulty across items (e.g., they still find fractions items harder than division items in a math test). In some cases, we may fall back to *metric invariance*, in which items show different difficulties across groups (e.g., $7\times9$ is harder for one group than another), or *configural invariance*, in which items show different degrees of ability discrimination as well (e.g., if some problem types are unfamiliar to children in one group and so do not discriminate between high and low ability children). These models allow us to begin to address the second challenge (variability across contexts) posed above.

<!-- https://www.tandfonline.com/doi/10.1080/00273171.2024.2396148 -->

In what follows, we begin by describing the nine LEVANTE core tasks, organized by construct. We then discuss the process of translation and adaptation that produced the Spanish and German versions of these tasks from the original English source. Next we describe our pilot data collection efforts in Colombia, Germany, and Canada. We subsequently present our IRT-based scoring techniques and the results of multi-group comparison, in some cases after selectively removing items that show poor performance. Using scores from these analyses, we then present evidence on the reliability and validity of the tasks, recognizing that in many cases these tasks are still under construction and we anticipate increases in reliability as we iteratively improve items. 

We end by discussing future plans for further internationalization and downward extension of the tasks. Critically, LEVANTE embraces open science values, aiming to create measures and data that are permissively licensed and reusable and extensible by the international research community. These values address our final challenge posed above: The aim of LEVANTE is to minimize barriers to reuse, accelerating progress towards a global science of learning and development. 

# The LEVANTE Core Tasks

```{r load-scores}
all_scores <- read_rds(here("02_scoring_outputs","scores","scores_combined.rds")) |>
  group_by(user_id, item_task) |>
  arrange(time_started) |>
  mutate(run_number = 1:n(),
         age_gap = age - age[1]) |>
  ungroup() |>
  mutate(site_label = site |> fct_recode(!!!site_labels)) |>
  mutate(task = task |> str_to_title() |> fct_inorder(),
         task_category = task_category |> str_to_title() |> fct_inorder())

task_map <- all_scores |> distinct(item_task, task)

included_scores <- all_scores |>
  filter(!is.na(age), age >= 5, age <= 12) |>
  filter(run_number == 1 | (run_number == 2 & age_gap > 1/6))

coretask_scores <- included_scores |>
  filter(item_task %in% core_tasks)

coretask_firstrun_scores <- coretask_scores |>
  filter(run_number == 1)
```

```{r time-summary}
# task_time_summary <- read_csv(here("03_summaries/tables/task_time_summary.csv"))
task_time_summary <- read_rds(here("03_summaries","tables",
                                   "task_time_summary.rds"))

task_time_table <- task_time_summary |>
  filter(item_task %in% core_tasks) |>
  mutate(site_label = site |> fct_recode(!!!site_labels, "All" = "all"),
         adaptive = if_else(is_cat, "Adaptive", "Non-adaptive")) |>
  left_join(task_map) |>
  select(task, site_label, adaptive, median_diff) |>
  pivot_wider(names_from = adaptive, values_from = median_diff) |>
  mutate(pct = 100 * as.numeric(Adaptive) / as.numeric(`Non-adaptive`),
         `Non-adaptive` = sprintf("%.2f", `Non-adaptive`),
         Adaptive = if_else(is.na(Adaptive), NA, as.character(glue("{Adaptive} ({round(pct, 0)}%)")))) |>
  select(-pct) |>
  pivot_longer(names_to = "adaptive", values_to = "median_diff", cols = c(`Non-adaptive`, `Adaptive`)) |>
  pivot_wider(names_from = c(adaptive, site_label), values_from = median_diff) |>
  select(task, starts_with("Non"), starts_with("Adaptive")) |>
  arrange(task)

# task_time_table |>
#   group
  # pivot_wider(names_from = adaptive, values_from = median_diff) |>
  # mutate(pct = 100 * as.numeric(Adaptive) / as.numeric(`Non-adaptive`))
```

## General task properties

The LEVANTE core tasks are brief behavioral assessments that can be delivered in a web browser on a tablet or laptop, with responses made via touchscreen, keyboard, or mouse. Because administration formats vary, the tasks prioritize response accuracy rather than reaction time (and are therefore mostly untimed). However, reaction times are still recorded for all tasks. The tasks are designed for simplicity and clarity so as to be accessible to children across a wide age range, and so with only modest exceptions, nearly all are in the format of a multi-alternative forced choice with a maximum of four choices, and involve colored illustrations[^1]. This uniformity of format means that in most cases instructions can be short and easy to understand, minimizing delays when the tasks are given in sequence as a battery. @fig-tasks shows screenshots from each of the tasks. Below we briefly present each task.

[^1]: The vocabulary task uses photographs, and the math task shows only text.

![Screenshots from each of the nine LEVANTE core tasks. Top row, from left to right: Sentence Understanding, Vocabulary, Hearts and Flowers; middle row: Memory, Same and Different, Math; bottom row: Pattern Matching, Mental Rotation; Stories.](display/tasks.png){#fig-tasks}

Tasks were implemented using jsPsych [@de-leeuw2015] in a common framework. Illustrations were produced by hand, while auditory stimuli were generated using AI-based voice synthesis tools (<https://elevenlabs.io>), which enabled rapid and iterative revisions without the need for rerecording. The tasks generally share a set of usability features including simple instructions, a small number of practice trials, and buttons to replay audio.

All tasks are available for demonstration purposes at <https://researcher.levante-network.org>. Source code for the tasks is available at <https://github.com/levante-framework/core-tasks>. Task code and assets are licensed CC-BY-NC 4.0 for non-commercial reuse (including educational use by not-for-profit and governmental entities) with appropriate attribution. Please see the license file available in the repository for more details. Once the LEVANTE researcher management dashboard is completed, interested researchers will be able to create accounts and use the tasks in their own research. 

::: {.landscape}
```{r}
#| label: tbl-durations
#| tbl-cap: "Median task durations (minutes) for non-adaptive (left) and adaptive (right) versions of each task, within across all sites and within each site. Adaptive durations include in parentheses what percentage that adaptive duration is of the corresponding non-adaptive duration."
#| cache: false

task_time_table |>
  rename(Task = task) |>
  flextable() |>
  separate_header() |>
  align(align = "right", part = "body") |>
  align(align = "left", j = 1, part = "all") |>
  flextable::autofit(add_w = 0)
```
:::

General task lengths are described below and average task durations from pilots (for both CAT and non-CAT versions) are given in @tbl-durations. Because of our interest in testing children across a wide range of ages, we intentionally included trials that we anticipated would be both very easy and very hard for children with the aim of deploying these items adaptively after we had gathered sufficient data. We wanted to avoid frustration for children, however, so during pilot testing, we experimented with a number of different stopping rules. In early iterations of pilot testing in the Colombia site, we ended tasks after three consecutive incorrect trials; we later modified this rule to end tasks after six consecutive incorrect trials. 

## Language 

### Sentence Understanding

The Test for Reception of Grammar (TROG) [@bishop1982] is a multiple-choice measure of receptive grammatical understanding. On each trial, the child hears a spoken sentence and is asked to select one of four pictures that best matches its meaning. The original test contained 20 blocks, each with four items assessing the same grammatical structure. In our adaptation of the original TROG (permissively licensed for reuse), we removed several items that no longer reflected contemporary cultural norms, particularly those involving gender stereotypes. In addition, based on early pilot testing showing that many trials were easy for older children, we added a set of several dozen more challenging sentences. All illustrations were remade with details intended to be accessible across a broad range of cultures. The task had 103 separate items.

### Vocabulary

The Vocabulary task was developed as a non-commercial, open alternative to tasks such as the Peabody Picture Vocabulary [@dunn1965] and the NIH Toolbox Picture Vocabulary Task [@nihtoolbox]; see @long_ma_tan_silverman_frank_yeatman_2025 for more details. In this task, children hear a target word read aloud and are shown four pictures. They must identify the picture that matches the word among three distractors. A first set of targets and distractors were sourced from the THINGS+ dataset [@hebart2019things, @stoinski2024thingsplus], specifically from the permissively-licensed subset of the data; in this item subset (108 items) each image had a semantically close and semantically far distractor as well as an unrelated distractor image (e.g., target word “acorn” with close distractor being a “coconut”, and the far and unrelated distractors being “keys” and “laundry”).  Additional difficult items were included beyond those analyzed in @long_ma_tan_silverman_frank_yeatman_2025. These more difficult items were sourced from translated items from an open-sourced Dutch receptive vocabulary assessment tool, the DAIVT [@bousard2021dutch]; we also manually constructed several of our own items to reach a total of 170.

## Math

We developed the Math task based on the EGMA (Early Grade Mathematics Assessment) [@platas2014]. This short, paper-and-pencil assessment is widely administered in international contexts for children ages 5--8 and includes number identification, number comparison, missing number, addition, and subtraction sub-tests. In our adaptation, we increased the breadth of the initial item bank, added multiplication, division, and fractions items to extend up to approximately 12 years. We also included number line identification problems in which children were asked to place a marker on a number line across a range of scales (including simple 0–10 trials, larger whole-number scales, and a fraction scale). Number line tasks of this type are known to be strongly associated with mathematical ability [@schneider2018]. The primary version of this task included 275 items (though no child saw all items) and the secondary version for retest purposes included an additional 237 (512 items in total). 

## Reasoning

### Pattern Matching

In classic matrix reasoning tasks, participants see a set of abstract figures in a grid format with a missing cell and must choose the most appropriate figure to complete the grid. Matrix reasoning scores are highly correlated with general cognitive ability and educational attainment [e.g., @roth2015]. Here we make use of the Mars-IB matrix reasoning stimuli, an open-source, permissively-licensed set of 80 matrix reasoning problems with retest variants that have been normed with a large sample of adolescents and adults [@chierchia2019].
 
### Shape Rotation

Mental rotation refers to the ability to imagine the rotation of objects in space, which is strongly related to mathematics and reasoning abilities [@xie2020]. In our version of the mental rotation task [based on @frick2013], children are presented with a target item (e.g., a picture of a rabbit). Then, they are presented with two comparison items (e.g., two silhouettes of rabbits). One of the comparison items will match the target item when rotated; the other comparison item will not match the target item when rotated. The child must select the comparison item that matches the target item. Both accuracy and reaction time are measured. Across trials, the degree of rotation varies (e.g., 40, 80, 120, 160 degrees), affecting children’s accuracy and reaction time. We included both two-dimensional duck and rabbit stimuli as well as harder three-dimensional geometric figures from the original stimulus set developed by @shepard1971.

## Executive Function

### Same Different Selection

The Same Different Selection (SDS) task is designed to assess cognitive flexibility in children [@obradovic2025sds]. It incorporates elements from the “Something’s the Same” task [@willoughby2011] and “Flexible Item Selection Task” [@jacques2001] as initial task blocks and extends these tasks by including more advanced blocks that increase the number of conceptual dimensions, values per dimension, and number of presented cards. Children are presented with sets of cards that show geometric figures that vary along multiple dimensions, such as shape, color, size, and number. They are required to identify similarities and differences between cards based on these dimensions (e.g., choosing a pair of cards with figures that share the same color and then choosing a pair of cards that share the same number of figures), by engaging their ability to shift attention from previously identified dimensions and identify new matching dimensions criteria. The version of the task we adapted included successively more difficult blocks of trials in which children were asked to identify two, three, or four pairs of items that were the same in different ways (e.g., matching on different dimensions).

### Hearts and Flowers

The Hearts and Flowers task assesses inhibitory control and cognitive flexibility [@davidson2006]. We modeled the task design after the AMES tablet task version which has been tested in many global settings including Ghana and Côte d'Ivoire [@khan2024modeling, @ahmed2022directly]. Participants respond according to stimulus type: pressing a key on the same side as a heart (congruent rule), and on the opposite side for a flower (incongruent rule). The task includes three blocks: congruent (hearts only), incongruent (flowers only), and mixed (hearts and flowers). The congruent block serves as a baseline with minimal executive demands. Participants in our version of the task are encouraged to respond as quickly as possible but -- due to variation in administration platform and in contrast to some versions of the task -- we did not provide a hard time limit on each trial, though we scored responses slower than 3000 ms as incorrect. 

### Memory

The Corsi Block task is a widely-used measure of visuospatial short-term and working memory [@corsi1972]. In our version of the task modeled after the AMES tablet task version [@obradovic2025motivation], the child sees a grid of squares; during each trial, a subset of squares lights up one at a time in a specific sequence. The child is required to reproduce the sequence by touching the squares: in the first block, the sequence is reproduced in the same order and in the second block, it is reproduced backwards. The task begins with short sequences (e.g., two items) and gradually increases in difficulty (up to seven items) until the child fails two sequences of the same length. We used both 2x2 and 3x3 grids in our pilot testing, with relatively similar results.

## Social Cognition

### Stories

Assessing social cognition across a wide range of ages is a challenge, and relatively few instruments show good reliability and validity across not just early childhood but older children as well [cf. @heise2025]. In addition, due to the lengthy nature of vignette-based assessments, it was challenging to identify an assessment that would fit in our allotted time. We chose to adapt and re-illustrate a storybook task developed by @sotomayor-enriquez2024 in which children hear stories with multiple social reasoning questions embedded, including questions about true and false beliefs, deception, and moral reasoning. Using the published data from this task, we selected a set of items that showed good psychometric properties and that we believed that we adapted to most likely be cross-culturally valid (e.g., with items commonly found across many cultures), and then supplemented these with additional trials tapping emotional reasoning. The result was a set of six stories, each with approximately five forced-choice social reasoning questions embedded within each story. We then developed two sets of alternative retest stories, in each case using the same structure as an existing story but substituting new details and characters. 

## Translation and adaptation

Initially designed in English, the core tasks were subsequently adapted for Spanish and German following a set of internationalization procedures in collaboration with researchers at the sites collecting pilot data in Colombia and Germany. We began with AI-generated translations of all verbal materials, including task instructions, items, and other key terminology such as encouragement phrases that appear throughout the core tasks; we used the DeepL platform for these initial translations. A professional translator then reviewed and edited the AI translations for general grammatical correctness and appropriateness. Materials were next reviewed by a researcher who was a native speaker of the relevant language and dialect, confirming that the materials were clear and appropriate for the specific task. We then conducted back translation using AI translation tools to verify semantic and conceptual equivalence and to flag items needing further review by native speakers. For tasks evaluating language skills (Vocabulary and Sentence Understanding), we additionally recruited PhD-level linguists with expertise in language acquisition for both Spanish and German to review items and make recommendations about relevant changes or additions to the item set for that language. Additional language adaptation work for these two tasks is ongoing based on the results of our pilot data collection. 

# Pilot Data Collection

```{r compute_counts}
percentify <- \(s) as.character(glue("{sprintf('%.1f', s)}%"))

run_data <- read_rds(here("01_fetched_data/run_data.rds"))
runs_nonempty <- read_rds(here("01_fetched_data/runs_nonempty.rds"))
runs_filtered <- read_rds(here("01_fetched_data/runs_filtered.rds"))

# all runs
run_counts <- run_data |> count(site, task_id, name = "n_total_runs")

# runs with any trials
run_nonempty_counts <- runs_nonempty |> count(site, task_id, name = "n_nonempty_runs")

# runs filtered to completed, no straightlining, first in admin
run_filtered_counts <- runs_filtered |> count(site, task_id, name = "n_filtered_runs")

# runs in trial data (must have item info)
# trial_data <- read_rds(here(glue("01_fetched_data/trial_data.rds")))
# trial_run_counts <- trial_data |> distinct(site, task_id, run_id) |> count(site, task_id, name = "n_trials_runs")

# runs in filtered trial data (both above filters + not all invalid trials)
task_data <- read_rds(here(glue("01_fetched_data/task_data_nested.rds"))) |> unnest(data)
trial_filtered_run_counts <- task_data |> distinct(site, task_id, run_id) |> count(site, task_id, name = "n_trials_filtered_runs")

# scores (above runs + non-missing age)
score_counts <- all_scores |> filter(item_task != "ha") |> count(site, task_id, name = "n_scores")

# included scores (above scores + age in range + first run or large enough age gap)
included_score_counts <- included_scores |> filter(item_task != "ha") |> count(site, task_id, name = "n_included_scores")

# combined_counts <- list(run_counts, trial_run_counts, run_filtered_counts, trial_filtered_run_counts, score_counts, included_score_counts) |>
counts <- list(run_counts, run_nonempty_counts, run_filtered_counts, trial_filtered_run_counts, score_counts, included_score_counts) |>
  reduce(left_join) |>
  left_join(all_scores |> distinct(task_id, item_task, task) |> filter(item_task != "ha")) |>
  filter(item_task %in% core_tasks) |>
  select(-task_id, -item_task) |>
  relocate(task, .before = everything()) |>
  arrange(task) |>
  mutate(site = site |> fct_recode(!!!site_labels))

# write_csv(counts, here("04_paper/display/counts.csv"))

# counts_long <- combined_counts |>
#   pivot_longer(starts_with("n_"), names_to = "count", names_prefix = "n_", values_to = "n") |>
#   mutate(count = count |> fct_inorder())

# anti_join(
# all_scores |> filter(item_task != "ha") |> select(site, task_id, run_id),
# task_data |> distinct(site, task_id, run_id)
# )
# 
# lost_runs <- anti_join(
#   task_data |> filter(item_task %in% irt_tasks, item_task != "ha") |> distinct(site, task_id, run_id),
#   all_scores |> select(site, task_id, run_id)
# ) 
  
  # filter(site == "pilot_western_ca") |>
  # filter(timestamp == min(timestamp))
# run_data |>
#   semi_join(lost_runs) |>
#   filter(!is.na(age)) |>
#   filter(time_started == min(time_started)) |> pull(time_started)
#   pull(run_id) |> pluck(1)

site_counts <- counts |>
  group_by(site) |>
  summarise(across(starts_with("n_"), sum)) |>
  pivot_longer(cols = -site,
               names_to = "count", values_to = "n", names_prefix = "n_")

site_diffs <- site_counts |>
  group_by(site) |>
  mutate(diff = lag(n) - n,
         pct_diff = diff / max(n) * 100,
         excluded = glue("{diff} [{percentify(pct_diff)}]"),
         comparison = paste(lag(count), "-", count)) |>
  ungroup() |>
  filter(count != "total_runs") |>
  select(site, count = comparison, n = excluded)

site_endpoints <- site_counts |>
  group_by(site) |>
  slice(1, n()) |>
  ungroup() |>
  mutate(n = as.character(n))

site_table <- bind_rows(site_endpoints, site_diffs) |>
  pivot_wider(names_from = site, values_from = n) |>
  mutate(count = count |> fct_inorder() |> fct_relevel("included_scores", after = Inf)) |>
  arrange(count) |>
  mutate(count = count |> fct_recode(
    "Total number of runs" = "total_runs",
    "Excluded for not having any trial data" = "total_runs - nonempty_runs",
    "Excluded for being incomplete, invalid, or duplicate" = "nonempty_runs - filtered_runs",
    "Excluded for missing item metadata" = "filtered_runs - trials_filtered_runs",
    "Excluded for missing age" = "trials_filtered_runs - scores",
    "Excluded for age out of range or too short retest gap" = "scores - included_scores",
    "Final number of scored runs" = "included_scores"
  )) |>
  rename(Count = count)

site_totals <- site_counts |> filter(count == "total_runs") |> select(site, n) |> deframe()

user_totals <- run_data |>
  group_by(site) |>
  summarise(n_users = n_distinct(user_id)) |>
  mutate(site = site |> fct_recode(!!!site_labels))

user_score_counts <- coretask_scores |>
  group_by(site) |>
  summarise(n_users = n_distinct(user_id),
            n_runs = n_distinct(run_id)) |>
  mutate(site = site |> fct_recode(!!!site_labels))

collapse_value_list <- \(l) l |> imap(\(v, n) paste(n, v, sep = ": ")) |> paste(collapse = ", ")

get_count_row <- \(i) {
  site_table |> slice(i) |> select(-Count) |> collapse_value_list()
  # imap(\(v, n) paste(n, v, sep = ": ")) |> paste(collapse = ", ")
}

trial_data_subset <- read_rds(here(glue("01_fetched_data/trial_data_subset.rds")))
trial_data_filtered <- read_rds(here(glue("01_fetched_data/trial_data_filtered.rds")))

trial_counts <- left_join(
  trial_data_subset |> count(site, item_task, name = "n_trials"),
  trial_data_filtered |> count(site, item_task, name = "n_included_trials")
) |>
  filter(item_task %in% core_tasks) |>
  mutate(site = site |> fct_recode(!!!site_labels),
         n_excluded_trials = n_trials - n_included_trials,
         pct_excluded = n_excluded_trials / n_trials * 100) |>
  mutate(excluded = glue("{n_excluded_trials} [{percentify(pct_excluded)}]"))

task_trial_counts <- trial_counts |>
  group_by(item_task) |>
  summarise(across(where(is.integer), sum)) |>
  mutate(pct_excluded = (n_trials - n_included_trials) / n_trials * 100) |>
  mutate(excluded = glue("{n_excluded_trials} [{percentify(pct_excluded)}]"))

site_trial_counts <- trial_counts |>
  group_by(site) |>
  summarise(across(where(is.integer), sum)) |>
  mutate(pct_excluded = (n_trials - n_included_trials) / n_trials * 100) |>
  mutate(excluded = glue("{n_excluded_trials} [{percentify(pct_excluded)}]"))

total_trial_counts <- trial_counts |>
  group_by() |>
  summarise(across(where(is.numeric), sum)) |>
  mutate(pct_excluded = (n_trials - n_included_trials) / n_trials * 100) |>
  mutate(excluded = glue("{n_excluded_trials} [{percentify(pct_excluded)}]"))

```


The pilot site partnership was an integral component of the core task development. Across three countries and languages, pilot sites provided collaboration on task adaptation and functionality, iterative task testing, and diverse settings for infrastructure deployment. Data collected from pilot sites allowed analyses of construct validity, measurement invariance, test-retest reliability, parameters for computer adaptive testing, and a demonstration of developmental growth measured by the tasks across constructs. Because we continued developing tasks continuously during the ~18-month pilot process, we anticipate that our reported reliability and validity estimates are an underestimate of the current task performance. Our analyses average over early versions of the tasks that in some cases had instructions or items that were later revised for clarity. 

A key feature of the LEVANTE framework is that all data are fully de-identified, minimising legal and ethical barriers to data sharing. No sociodemographic information or other identifying details about the child are entered into the LEVANTE dashboard by participating sites, and ages are recorded only to the nearest month [@frank2025]. We anticipate that sites will add sociodemographic information at a later stage via an app currently under development, which is designed to ensure that individuals cannot be statistically re-identified (e.g., due to rare combinations of characteristics within a community). For this reason, here we provide only minimal demographic characterization and analysis of the children from our pilot sites. @fig-age-hist provides the distribution of ages for each site. 

```{r}
#| label: fig-age-hist
#| fig-cap: "Overall distributions of ages for each site."
#| fig-height: 3

ages <- coretask_scores |>
  group_by(site, site_label, dataset, user_id) |>
  summarise(age = min(age), 
            n_runs = n())

ggplot(ages, aes(x = age, fill = site)) + 
  facet_wrap(vars(site_label)) + 
  geom_histogram(binwidth = 1, color = "white") +
  scale_fill_solarized(guide = "none") +
  scale_y_continuous(expand = expansion(0, 0)) +
  labs(x = "Age (years)", y = "Number of children")
```

We use "run" to represent an instance of a child attempting a single task. Across all tasks, the sites collected a total of `{r} sum(site_totals)` runs (`{r} get_count_row(1)`) from `{r} sum(user_totals$n_users)` unique children (`{r} user_totals |> deframe() |> collapse_value_list()`), of which some runs were excluded for not having any associated trial data (`{r} get_count_row(2)`), for being incomplete, invalid, or duplicate (`{r} get_count_row(3)`), for missing item metadata (`{r} get_count_row(4)`), for missing age (`{r} get_count_row(5)`), or for age being out of range or having too short (<0.6 months) of a retest gap (`{r} get_count_row(6)`), resulting in a total of `{r} sum(user_score_counts$n_runs)` scored runs (`{r} user_score_counts |> select(site, n_runs) |> deframe() |> collapse_value_list()`) from `{r} sum(user_score_counts$n_users)` unique children (`{r} user_score_counts |> select(site, n_users) |> deframe() |> collapse_value_list()`). 

Within the included runs, there were a total of `{r} total_trial_counts$n_trials` trials (`{r} site_trial_counts |> select(site, n_trials) |> deframe() |> collapse_value_list()`), of which `{r} total_trial_counts$excluded` were excluded (`{r} site_trial_counts |> select(site, excluded) |> deframe() |> collapse_value_list()`) due to too fast (<100ms) or too slow (>30s) reaction times. For reasons of space we do not disaggregate these exclusions by task, but note that the task with the highest exclusion rate was Pattern Matching (`{r} percentify(max(task_trial_counts$pct_excluded))` trials excluded), while all the other tasks ranged between `{r} percentify(min(task_trial_counts$pct_excluded))` and `{r} percentify(sort(task_trial_counts$pct_excluded, decreasing = TRUE)[2])` trials excluded.

## uniandes-co

The Universidad de Los Andes conducted school-based data collection across four schools in Bogotá and three schools in the rural areas of Caquetá and Boyacá, Colombia. By partnering with schools, the research team oversaw task data collection with children using a group testing format on tablets provided by the team. Children were tested in groups, with larger groups for older children and smaller groups for younger children. The uniandes-co site collected data from `{r} user_totals$n_users[user_totals$site == "uniandes-co"]` children ages 5--12 years. Due to time constraints for school-based administration, children typically completed a subset of the tasks. 

SITE-DESCRIPTION PLACEHOLDER: INSERT 2-3 SENTENCES CHARACTERIZING SITE POPULATION AND HOW THE SAMPLE WAS SELECTED.

## mpieva-de

The Max Planck Institute for Evolutionary Anthropology collected data using family-based remote data collection. Participants were recruited via an existing database of Leipzig families by first contacting families via telephone with an invitation to participate in the study. Upon agreement, families received an email with information to log into the LEVANTE system and complete the tasks assigned to them at home using their own computer or tablet. This site provided retest data with follow-up assignments sent to families 4--5 months after initial completion of the tasks. The mpieva-de site collected data from `r user_totals$n_users[user_totals$site == "mpieva-de"]` children, ages 5--12 years. 

SITE-DESCRIPTION PLACEHOLDER: INSERT 2-3 SENTENCES CHARACTERIZING SITE POPULATION AND HOW THE SAMPLE WAS SELECTED.

## western-ca

Partners at Western University (Ontario) recruited participants from the local community, primarily through online outreach (Facebook advertisements and community group posts), supplemented by posters on neighborhood mailboxes and notices shared via local school newsletters. Children were tested individually by a researcher in a dedicated testing space on campus. The western-ca site collected data from `r user_totals$n_users[user_totals$site == "western-ca"]` children, ages 5-12 years. 

SITE-DESCRIPTION PLACEHOLDER: INSERT 2-3 SENTENCES CHARACTERIZING SITE POPULATION AND HOW THE SAMPLE WAS SELECTED.

# Scoring

Our goal for each task was to extract administration-level ability scores with the highest possible reliability and validity. To do so, we first conducted heuristic item screening to remove poorly functioning items, then fit a sequence of multi-group IRT models to the retained items to estimate latent ability scores. These analyses provided the foundation for the reliability, developmental, and validity results described in subsequent sections.

## Heuristic item pruning and data checking

Prior to fitting psychometric models, we examined item-level performance summaries for each task within each site to identify items that were functioning unusually poorly (e.g., showing average accuracy near or below chance). We also reviewed classical diagnostics such as proportion correct, item–total correlations, and missingness rates to flag items that showed little discrimination or appeared anomalous. In some cases this was due to potential translation issues; in others it reflected aspects of item design. We manually excluded these items from further processing (N = 35; 23 items in Stories, 1--4 items in each of Same & Different, Math, Pattern Matching, Sentence Understanding, and Vocabulary). In one case, this process helped us identify a bug in the scoring of data for three tasks; we excluded these observations, which were made during a two day period of testing for the mpieva-de site. The resulting cleaned calibration dataset formed the basis for all subsequent IRT estimation and invariance analyses.

## Multi-group IRT model selection

Following item screening, we evaluated measurement invariance across sites using nested multi-group IRT models to assess the comparability of latent ability estimates. The conventional hierarchy of factor-analytic invariance—configural → metric → scalar—was implemented in the IRT framework as successive equality constraints on item parameters (see @tbl-invariance-explainer for correspondence and interpretation). Configural invariance reflects a shared latent structure, metric invariance constrains item discriminations, and scalar invariance constrains both discriminations and difficulties across sites.

For each task, we compared unidimensional Rasch and two-parameter logistic (2PL) models under these nested invariance constraints. All models were estimated using the `mirt` R package [@mirt] via marginal maximum likelihood using 500 quadrature points, with expected a posteriori (EAP) scoring. Overlapping items across sites served as anchors to establish a common scale; non-overlapping items were freely estimated within the same multi-group calibration. Model convergence and identification were verified by ensuring stable log-likelihoods and Hessian condition numbers below $10^3$.

Model selection was based on the Bayesian Information Criterion (BIC), with the final model for each task representing the most parsimonious combination of model type and invariance constraints that adequately reproduced cross-site response patterns. Across domains, Rasch models generally provided the best fit, whereas 2PL models were preferred for Same and Different and Shape Rotation, which showed greater variability in item discrimination.

As shown in @tbl-task-rxx, six tasks -- Hearts and Flowers, Sentence Understanding, Pattern Matching, Shape Rotation, Math, and Memory -- reached scalar invariance, indicating that both discrimination and difficulty parameters generalized across sites. One task -- Same and Different -- achieved metric invariance, indicating consistent discriminations but some variation in item difficulty. Two tasks -- Stories and Vocabulary -- showed configural invariance, suggesting a shared latent construct with certain parameters differing across groups.

Overall, these results suggest that most tasks showed at least partial measurement equivalence across sites. The final multi-group calibrations provided a common metric for subsequent ability estimation.

## Score computation

Individual ability scores were estimated from the final multi-group calibrations using the expected-a-posteriori (EAP) method. Scores are expressed on a common latent scale across sites, allowing for direct cross-site comparisons and forming the basis for the developmental, reliability, and validity analyses that follow.

# Psychometric properties of tasks

## Developmental change

Across all nine tasks we observed positive age–ability trends. The smoothed lines in @fig-age-trends suggest that between-site differences primarily reflect overall level differences in ability, rather than differences in the rate at which ability increases with age. Growth appeared steepest for Math and Hearts and Flowers, moderate for Vocabulary, Memory, and Stories, and shallower for Shape Rotation and Sentence Understanding. See Appendix for predicted ability vs age by site and task @fig-predicted-age-slopes. 

To quantify developmental change, we fitted separate ordinary least-squares models predicting ability scores (IRT units) from centered age with site fixed effects and an age × site interaction, allowing slopes to vary by site. As summarized in @tbl-slopes, age-related slopes were positive for all tasks and sites, indicating that older children performed better on every measure (see Figure @fig-em-trends in Appendices for visual representation). 

In addition, we calculated per-site zero-order correlations between age and IRT ability scores. These correlations were uniformly positive (r ≈ .40–.70) and of moderate magnitude across tasks, consistent with the regression estimates. The close correspondence between the linear slopes and the smooth trends suggests that, within the studied age range, developmental gains are approximately linear and broadly comparable across cultural contexts.

::: {.landscape}
```{r}
#| label: tbl-invariance-explainer
#| tbl-cap: "Measurement invariance across factor analysis and item response theory."
#| ft.arraystretch: 1.2

# read and lightly normalize columns
inv_explainer <- read_csv(here("04_paper", "display", "invariance.csv")) |>
  mutate(across(everything(), \(v) replace_na(v, "–"))) |>
  relocate(mirt, .after = model) #|>
  # mutate(test = "``bem''")

# build table (mirrors tbl-task-rxx style)
inv_explainer |>
  flextable() |>
  set_header_labels(
    invariance = "Invariance",
    model = "Model",
    mirt = "mirt invariance param",
    fa = "Factor-analytic analog",
    irt = "IRT explanation",
    interpretation = "Practical interpretation"
  ) |>
  width(j = "invariance", width = 0.65) |>
  width(j = "model", width = 0.45) |>
  width(j = "mirt", width = 1.3) |>
  width(j = "fa", width = 1.8) |>
  width(j = "irt", width = 2.3) |>
  width(j = "interpretation", width = 2) |>
  align(align = "left", part = "all") |>
  valign(val = "top", part = "all") |>
  fontsize(size = 10, part = "all") |>
  font(j = "mirt", fontname = "Monaco") |>
  fontsize(j = "mirt", size = 8)
```
:::
```{r}
task_lab_map <- c(
  hf="Hearts and Flowers", mg="Memory", sds="Same and Different",
  matrix="Pattern Matching", mrot="Shape Rotation",
  trog="Sentence Understanding", vocab="Vocabulary",
  math="Math", tom="Stories"
)

task_meta <- coretask_scores %>% distinct(item_task, task, task_category)
task_ids  <- unique(coretask_scores$item_task)

prep_task <- function(df) df %>%
  mutate(site = factor(site),
         age_c = as.numeric(scale(age, scale = FALSE)))

fmt_p <- function(p) case_when(
  is.na(p) ~ NA_character_,
  p < .001 ~ "<0.001",
  TRUE     ~ sprintf("%.3f", p)
)
```

```{r}
#| label: fig-age-trends
#| fig-cap: "Age related trends by task."
#| fig-width: 8
#| fig-height: 9

# remove CAT data
fixed_only <- coretask_scores %>% filter(adaptive == FALSE)

ggplot(fixed_only, aes(age, metric_value)) +
  ggh4x::facet_nested_wrap(vars(task_category, task),
                           nest_line = element_line(), solo_line = TRUE,
                           axes = "x", scales = "free_y") +
  geom_smooth(aes(colour = site_label, group = site_label),
              method = "gam", formula = y ~ s(x, bs = "re"), se = FALSE) +
  geom_point(aes(colour = site_label), alpha = 0.3, size = 1) +
  scale_x_continuous(breaks = seq(6, 14, 2)) +
  .scale_colour_site() +
  guides(colour = guide_legend(override.aes = list(shape = NA, linetype = 1, linewidth = 2))) +
  labs(x = "Age (years)", y = "Ability (IRT score)", colour = "Site") +
  theme(legend.position = "bottom")
```

::: {.landscape}
```{r}
#| label: tbl-slopes
#| tbl-cap: "Age-Related Change in Performance: Task Grand Means (GM), Site Deviations (Δ), and Age–Ability Correlations (r)."
#| ft.arraystretch: 1

analyze_task_slopes <- function(task_name) {
  d <- coretask_scores %>% filter(item_task == task_name)
  d$site  <- as.factor(d$site)
  d$age_c <- scale(d$age, scale = FALSE)[, 1]
  contrasts(d$site) <- contr.sum(nlevels(d$site))

  fit <- lm(metric_value ~ age_c * site, data = d)

  # grand-mean slope (main effect of age_c under sum-to-zero contrasts)
  cs <- coef(summary(fit))
  mean_slope <- unname(cs["age_c", "Estimate"])
  mean_se    <- unname(cs["age_c", "Std. Error"])
  mean_t     <- unname(cs["age_c", "t value"])
  mean_p     <- unname(cs["age_c", "Pr(>|t|)"])
  mean_df    <- df.residual(fit)

  # site deviations from the grand-mean slope
  tr_site <- emtrends(fit, ~ site, var = "age_c")
  dev_tab <- contrast(tr_site, "eff") %>%
    summary(infer = c(TRUE, TRUE)) %>%
    as_tibble() %>%
    transmute(
      site            = sub("\\s*effect$", "", contrast),
      Slope_Deviation = estimate,
      SE              = SE,
      t               = t.ratio,
      df              = df,
      p_value         = p.value
    )

  # site-wise r(age, ability)
  corr_tab <- d %>%
    filter(!is.na(age_c), !is.na(metric_value)) %>%
    mutate(site = as.character(site)) %>%
    group_by(site) %>%
    summarise(
      N = n(),
      r = if (n() >= 3 && sd(age_c) > 0 && sd(metric_value) > 0)
            cor(age_c, metric_value) else NA_real_,
      .groups = "drop"
    ) %>%
    mutate(
      r_t  = ifelse(!is.na(r) & N >= 3 & abs(r) < 1, r * sqrt((N - 2)/(1 - r^2)), NA_real_),
      r_df = ifelse(!is.na(r_t), N - 2, NA_real_),
      r_p  = ifelse(!is.na(r_t), 2 * pt(-abs(r_t), df = r_df), NA_real_)
    )

  dev_tab %>%
    mutate(
      task      = task_name,
      Mean_Slope = mean_slope,
      Mean_SE    = mean_se,
      Mean_t     = mean_t,
      Mean_df    = mean_df,
      Mean_p     = mean_p
    ) %>%
    left_join(corr_tab, by = "site")
}

deviation_raw <- purrr::map_dfr(task_ids, analyze_task_slopes) %>%
  rename(item_task = task) %>%
  left_join(task_meta, by = "item_task") %>%
  mutate(Task = coalesce(task, recode(item_task, !!!task_lab_map)))

deviation_print <- deviation_raw %>%
  mutate(
    # fill any missing deviation p from t, df
    p_num = if_else(is.na(p_value) & !is.na(t) & !is.na(df),
                    2 * pt(-abs(t), df), p_value),
    `Deviation p-value`  = fmt_p(p_num),
    `Grand Mean p-value` = fmt_p(Mean_p),
    `r p-value`          = fmt_p(r_p)
  ) %>%
  transmute(
    Task,
    Site = forcats::fct_recode(site, !!!site_labels),
    `Grand Mean Slope (β)` = round(Mean_Slope, 2),
    `Grand Mean SE`        = round(Mean_SE, 2),
    `Grand Mean p-value`,
    `Deviation from Mean`  = round(Slope_Deviation, 2),
    `Deviation SE`         = round(SE, 2),
    `Deviation p-value`,
    N,
    `r (age, ability)`     = round(r, 2),
    `r p-value`
  )

flextable::flextable(deviation_print) |>
  flextable::set_header_labels(
    `Grand Mean Slope (β)` = "GM β",
    `Grand Mean SE`        = "GM SE",
    `Grand Mean p-value`   = "GM p",
    `Deviation from Mean`  = "Δ from GM",
    `Deviation SE`         = "Δ SE",
    `Deviation p-value`    = "Δ p",
    `r (age, ability)`     = "r(age,ability)",
    `r p-value`            = "r p"
  ) |>
  flextable::colformat_num(
    j = c("Grand Mean Slope (β)", "Grand Mean SE",
          "Deviation from Mean", "Deviation SE",
          "N", "r (age, ability)"),
    digits = 2
  ) |>
  flextable::align_text_col() |>
  flextable::autofit()
```
:::

```{r}
#| label: fig-em-trends
#| fig-cap: "Age Slopes by Task"
#| fig-appendix: true
#| fig-width: 8
#| fig-height: 9

slopes_by_task <- function(task_name) {
  d <- coretask_scores %>% filter(item_task == task_name)
  d$site  <- factor(d$site)
  d$age_c <- scale(d$age, scale = FALSE)[, 1]
  contrasts(d$site) <- contr.sum(nlevels(d$site))

  fit <- lm(metric_value ~ age_c * site, data = d)

  gm <- coef(summary(fit))["age_c", "Estimate"]

  tr_sum <- emtrends(fit, ~ site, var = "age_c") |>
    summary(infer = c(TRUE, TRUE)) |>
    as_tibble()

  slope_col <- intersect(names(tr_sum), c("estimate","trend","emmean","age_c.trend"))[1]
  if (is.na(slope_col)) stop("Could not find slope column in emtrends() summary")

  tr_sum |>
    transmute(
      item_task = task_name,
      site      = .data$site,
      slope     = .data[[slope_col]],
      SE        = .data$SE,
      df        = .data$df,
      t         = .data$t.ratio,
      p         = .data$p.value,
      lower     = .data$lower.CL,
      upper     = .data$upper.CL,
      gm_slope  = gm
    )
}

slopes_df <- purrr::map_dfr(task_ids, slopes_by_task) %>%                   
  dplyr::left_join(task_meta, by = "item_task") %>%
  dplyr::mutate(
    Site    = forcats::fct_recode(site, !!!site_labels),
    TaskLab = dplyr::coalesce(task, dplyr::recode(item_task, !!!task_lab_map)),
    TaskLab = factor(TaskLab, levels = unique(task_meta$task))               
  )

ggplot(slopes_df, aes(x = slope, y = forcats::fct_rev(Site), colour = Site)) +
  geom_vline(xintercept = 0, linetype = 3) +
  geom_vline(data = dplyr::distinct(slopes_df, TaskLab, gm_slope),
             aes(xintercept = gm_slope),
             linetype = 2, linewidth = 0.6, colour = "grey40") +
  geom_errorbarh(aes(xmin = lower, xmax = upper), width = 0) +
   geom_point(size = 2) +
  facet_wrap(~ TaskLab, scales = "free_x") +
  .scale_colour_site() +                                                     
  labs(x = "Age slope (β per year, estimated marginal trend)",
       y = NULL, colour = "Site",
       title = "Site-specific age slopes by task (EMTrends with 95% CI)",
       subtitle = "Dashed line: task grand-mean slope; dotted line: zero") +
  theme(legend.position = "bottom")
```

```{r}
#| label: fig-predicted-age-slopes
#| fig-cap: "Predicted Ability v Age"
#| fig-appendix: true
#| fig-width: 8
#| fig-height: 9

pred_curves_augment <- function(task_name) {
  d <- coretask_scores %>% filter(item_task == task_name)
  d$site  <- factor(d$site)
  d$age_c <- scale(d$age, scale = FALSE)[,1]
  mu_age  <- mean(d$age, na.rm = TRUE)

  fit <- lm(metric_value ~ age_c * site, data = d)

  grid <- expand.grid(
    age_c = seq(min(d$age_c, na.rm = TRUE),
                max(d$age_c, na.rm = TRUE),
                length.out = 100),
    site  = levels(d$site)
  )

  broom::augment(fit, newdata = grid, se_fit = TRUE) %>%
    transmute(
      item_task = task_name,
      site      = as.character(site),
      age       = age_c + mu_age,
      fit       = .fitted,
      lower     = .fitted - 1.96*.se.fit,
      upper     = .fitted + 1.96*.se.fit
    )
}

pred_all <- purrr::map_dfr(task_ids, pred_curves_augment) %>%
  dplyr::left_join(task_meta, by = "item_task") %>%
  dplyr::mutate(
    Site    = forcats::fct_recode(site, !!!site_labels),
    TaskLab = dplyr::coalesce(task, dplyr::recode(item_task, !!!task_lab_map)),
    TaskLab = factor(TaskLab, levels = unique(task_meta$task))   
  )

stopifnot(all(c("task_category","TaskLab") %in% names(pred_all)))

ggplot(pred_all, aes(age, fit, colour = Site)) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = Site),
              alpha = 0.12, colour = NA, show.legend = FALSE) +
  geom_line(linewidth = 1) +
  ggh4x::facet_nested_wrap(vars(task_category, TaskLab), scales = "free_y", axes = "x") +
  .scale_colour_site() +
  labs(x = "Age (years)", y = "Predicted ability (IRT)",
       title = "Model-predicted ability vs age by site and task",
       subtitle = "Lines are marginal predictions from lm(metric_value ~ age_c * site); ribbons = 95% CI") +
  theme(legend.position = "bottom")
```

## Reliability 

One goal for the LEVANTE core tasks is strong reliability both within an individual administration (across test items) and across multiple administrations separated in time. We report on each of these. 

### Marginal reliability

Marginal reliability, an IRT-based analogue to Cronbach’s α, quantifies the proportion of true-score variance captured by the test based on the item information function [@thissen2001test]. IRT models allow the computation of marginal reliability, which captures the inter-relatedness of items in an assessment and hence the proportion of variation attributed to shared variance. In multi-group IRT models, marginal reliabilities can be computed only for individual sub-models. @tbl-task-rxx shows marginal reliabilities for each task, reporting reliability for each sub-model for the best fitting model. Reliabilities were largely acceptable (>.6) [@nunnally1978psychometric], with the exception of the Stories and the Hearts and Flowers tasks, which had lower reliabilities for several sites.

```{r}
#| label: tbl-task-rxx
#| tbl-cap: "Marginal reliabilities for each task and site."

task_rxx <- read_rds(here("02_scoring_outputs/task_rxx.rds"))

trx <- task_rxx |>
  rename(item_task = task) |>
  left_join(task_map) |>
  select(task, site, itemtype, invariance, rxx) |>
  mutate(site = site |> fct_recode(!!!site_labels),
         itemtype = itemtype |> fct_recode("Rasch" = "rasch", "2PL" = "2pl"),
         invariance = invariance |> str_to_sentence()) |>
  arrange(task) |>
  pivot_wider(names_from = site, values_from = rxx,
              names_prefix = "Reliability_") |>
  rename(Task = task, Model = itemtype, Invariance = invariance)

trx |>
  flextable() |>
  separate_header() |>
  align_text_col() |>
  flextable::autofit(add_w = 0, add_h = 0)
```

```{r}
# prophecy <- function (n = 2, r) {
#   (n * r) /  (1 + (n - 1) * r)
# }

de_retest_scores <- coretask_scores |> 
  filter(site == "pilot_mpieva_de") |>
  group_by(user_id, task) |>
  arrange(user_id, task, age) |>
  filter(n() > 1) 

retest_wide <- de_retest_scores |>
  group_by(user_id, task) |>
  mutate(has_retest = any(age_gap > .05), 
         mean_age = mean(age), 
         age_gap = max(age_gap)) |>
  filter(has_retest) |>
  ungroup() |>  
  pivot_wider(
    id_cols = c(user_id, task_category, task, mean_age, age_gap),
    names_from = run_number,
    values_from = metric_value,
    names_prefix = "run_"
  ) |>
  select(user_id, task_category, task, age = mean_age, age_gap, run_1, run_2)

trt <- retest_wide |>
  group_by(task_category, task) |>
  summarise(test_retest_r = cor(run_1, run_2, use = "complete.obs"), 
            # prophecy_r = prophecy(n = 2, r = test_retest_r),
            n = n(), 
            age_gap = mean(age_gap) * 12) |>
  ungroup() |>
  arrange(task)
```

### Test-retest reliability

We were also interested in capturing the extent to which the LEVANTE tasks yield reliable signal across a delay. The mpieva-de pilot site was able to re-administer tasks to a subset of their original participant group with a retest interval of either 3 months (group 1) or 7 months (group 2) approximately. We used this re-administration as an opportunity to collect data with computer adaptive versions of many of the tasks (Sentence Understanding, Pattern Matching, Shape Rotation, Math, and Vocabulary); see below for further details of computer adaptive versions. In addition, for several tasks (Math, Stories, Pattern Matching), we used this opportunity to gather item data about new groups of retest items that were designed to match the content of the original items but vary in their surface forms. For the other tasks, we retested using the same items. 

@fig-retest shows the relations between time 1 and time 2 scores. Uptake for retest administrations varied between N=`r min(trt$n)` and N=`r max(trt$n)`; the low number of repeat administrations for Sentence Understanding reflects an error in which this task was not sent to younger children. Each figure panel also shows the test-retest correlation and sample size for the corresponding task. Most retest correlations were in the range of r = .5 -- .7, with two exceptions. The Sentence Understanding task had a smaller sample size with a more restricted age range and showed lower reliability within that range; and the Math task showed higher reliability than the other tasks. 

```{r}
#| label: fig-retest
#| fig-cap: "Test-retest correlations per task (in mpieva-de data)."
#| fig-width: 8
#| fig-height: 9

lim <- range(retest_wide$run_1, retest_wide$run_2, finite = TRUE)
lim <- range(pretty(lim))

ggplot(retest_wide, aes(run_1, run_2, colour = age)) +
  ggh4x::facet_nested_wrap(
    vars(task_category, task),
    scales = "fixed",
    axes   = "x",              # or "all" for both axes in every panel
    nest_line = element_line(),
    solo_line = TRUE
  ) +
  coord_equal(xlim = lim, ylim = lim, ratio = 1, expand = FALSE) +
  geom_abline(slope = 1, intercept = 0, colour = "grey60", linetype = "dotted") +
  geom_point(alpha = 0.8, size = 1.5) +
  geom_smooth(aes(group = 1), colour = "grey40", method = "lm", formula = y ~ x, linewidth = 0.6) +
  geom_label(
    data = trt, inherit.aes = FALSE,
    aes(label = sprintf("r = %.2f", test_retest_r)),
    x = lim[1], y = lim[2], hjust = "inward", vjust = "inward",
    size = 3.8, colour = "grey30", linewidth = 0
  ) +
  geom_label(
    data = trt, inherit.aes = FALSE,
    aes(label = sprintf("N = %s", n)),
    x = lim[2], y = lim[1], hjust = "inward", vjust = "inward",
    size = 3.8, colour = "grey30", linewidth = 0
  ) +
  viridis::scale_colour_viridis(limits = c(5, 12), end = 0.95) +
  labs(x = "Score 1", y = "Score 2", colour = "Age (years)") +
  theme(legend.position = "bottom")
```

\clearpage

## Validity

Assessment of validity for the core tasks was challenging, in part because of the precise issues that motivated us to create the task battery in the first place. No readily available, internationalized measures existed to compare to many of our tasks. Thus, for validation evidence, we rely on the overall construct structure of the core task battery as well as the relations to our gold standard measures for literacy and EF.

```{r prep-sem, include=FALSE}
# one row per user (metadata + task scores from first run)
meta <- coretask_firstrun_scores |>
  group_by(user_id) |>
  summarise(
    age        = min(age, na.rm = TRUE),
    site       = first(na.omit(site)),
    site_label = first(na.omit(site_label)),
    dataset    = first(na.omit(dataset)),
    .groups = "drop"
  )

# wide task scores (first run only)
wide_scores <- coretask_firstrun_scores |>
  select(user_id, item_task, metric_value) |>
  distinct() |>  # guard against accidental duplicates
  pivot_wider(
    names_from  = item_task,
    values_from = metric_value
  ) |>
  left_join(meta, by = "user_id") |>
  relocate(user_id, site, site_label, dataset, age)

# keep only the core task columns + metadata
vars_core <- c("math","matrix","mrot","sds","hf","mg","trog","vocab","tom")
wide_scores <- wide_scores |>
  select(user_id, site, site_label, dataset, age, any_of(vars_core))

# standardise
z <- function(x) (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
vars_core_std <- wide_scores |>
  mutate(across(all_of(vars_core), ~ z(as.numeric(.x))))
# str(vars_core_std[vars_core])
```

```{r sem-g, include=FALSE}
model_cov_no_age <- "
  # first-order factors
  reasoning =~ matrix + mrot
  ef        =~ hf + mg + sds
  language  =~ trog + vocab

  # covariances of factors with observed outcomes
  reasoning ~~ math + tom
  ef        ~~ math + tom
  language  ~~ math + tom

  # allow math and tom to covary
  math ~~ tom
"

fit_cov_no_age <- lavaan::sem(
  model_cov_no_age, data = vars_core_std,
  estimator = 'MLR', std.lv = TRUE, missing = 'fiml'
)
summary(fit_cov_no_age, fit.measures = TRUE, standardized = TRUE)

```

```{r sem-g-age, include=FALSE}
#| echo: false
#| results: "hide" 
# ensure log-age (centred) exists
vars_core_std$log_age_c <- scale(log(vars_core_std$age))[,1]

model_cov_age <- "
  # first-order factors (fix first loading to 1 for each factor)
  reasoning =~ 1*matrix + mrot
  ef        =~ 1*hf     + mg + sds
  language  =~ 1*trog   + vocab

  # age predicts factors
  reasoning ~ log_age_c
  ef        ~ log_age_c
  language  ~ log_age_c

  # age predicts outcomes
  math ~ log_age_c
  tom  ~ log_age_c

  # covariances of factors with observed outcomes
  reasoning ~~ math + tom
  ef        ~~ math + tom
  language  ~~ math + tom

  # (recommended) residual covariance between outcomes
  math ~~ tom
"

fit_cov_age <- lavaan::sem(
  model_cov_age, data = vars_core_std,
  estimator = 'MLR', missing = 'fiml',
  std.lv = FALSE, optim.method = 'nlminb',
  control = list(iter.max = 50000, rel.tol = 1e-6)
)
summary(fit_cov_age, fit.measures = TRUE, standardized = TRUE)
```

### Construct validity
We assessed construct validity using a structural equation model (SEM) specifying three first‐order latent factors—Reasoning, Executive Function, and Language—with each factor measured by its respective tasks and allowed to covary with Math and Stories (see @fig-semplot-core-tasks). Math and Stories were treated as observed endogenous outcomes because each construct had a single task; introducing one-indicator latent variables would require strong, unverifiable assumptions about reliability and error variance. Age was specified to predict each of the latent factors as well as the observed variables Math and Stories.

Model fit was imperfect -- CFI: `r round(lavaan::fitMeasures(fit_cov_age)["cfi"], 3)`, RMSEA: `r round(lavaan::fitMeasures(fit_cov_age)["rmsea"], 3)` RMSEA p-value: `r pval_fmt(lavaan::fitMeasures(fit_cov_age)["rmsea.pvalue"])`. In part we expect that lower model fit may be due to a combination of missing data and site-related differences, which are not accounted for in this specification; future work should pursue more complete models that account for these sources of variability.

Overall, factor loadings supported the intended three-factor structure (convergent validity), and age showed positive associations with each domain. The high inter-factor correlations indicate that much of the variance is shared across domains. Comparison with more parsimonious single-, two-, and three-factor SEMs that did not distinguish Math and Stories showed that none improved fit over the original model (see @tab-alt-models). We therefore retain the three-factor SEM incorporating age effects and treating Math and Stories as observed outcomes, on both conceptual and empirical grounds.

```{r}
#| label: fig-semplot-core-tasks
#| fig-cap: "SEM for core tasks"
#| fig-width: 8
#| fig-height: 9

source(here::here("03_summaries/modified_sem_plot.R")) # veronica helper

pe <- lavaan::parameterEstimates(fit_cov_age, standardized = TRUE)
latents  <- sort(unique(pe$lhs[pe$op == "=~"]))
indics   <- sort(unique(pe$rhs[pe$op == "=~"]))
extras   <- c("log_age_c","math","tom")
observed <- sort(union(indics, extras))

nodes <- tibble::tibble(
  name  = c(observed, latents),
  shape = c(rep("rect", length(observed)), rep("oval", length(latents)))
)

# Pretty labels
nodes$label <- dplyr::recode(
  nodes$name,
  matrix    = "Pattern\nMatching",
  mrot      = "Shape\nRotation",
  hf        = "Hearts and\nFlowers",
  mg        = "Memory",
  sds       = "Same and\nDifferent",
  trog      = "Sentence\nUnderstanding",
  tom       = "Stories",
  ef        = "Executive Function",
  log_age_c = "Age (log years)",
  .default  = stringr::str_to_title(nodes$name)
)

factor_names  <- c("reasoning","ef","language")
outcome_names <- c("math","tom")

edges <- pe %>%
  dplyr::filter(op %in% c("~","=~","~~")) %>% # hide math and tom residual covariance
  dplyr::filter(!(op == "~~" & ((lhs == "math" & rhs == "tom") |
                                (lhs == "tom"  & rhs == "math")))) %>%
  dplyr::mutate(keep = dplyr::case_when(
    op %in% c("~","=~") ~ TRUE,
    op == "~~" & lhs %in% factor_names & rhs %in% outcome_names ~ TRUE,
    op == "~~" & rhs %in% factor_names & lhs %in% outcome_names ~ TRUE,
    op == "~~" & lhs %in% factor_names & rhs %in% factor_names & lhs != rhs ~ TRUE,
    op == "~~" & lhs == rhs ~ TRUE,
    TRUE ~ FALSE
  )) %>%
  dplyr::filter(keep) %>%
  dplyr::transmute(
    from = dplyr::if_else(op == "~", rhs, lhs),
    to   = dplyr::if_else(op == "~", lhs, rhs),
    op, est, std.all, pvalue,
    arrow     = dplyr::if_else(op == "~~", "both", "last"),
    curved    = op == "~~" & lhs != rhs,
    curvature = dplyr::case_when(
      op == "~~" & lhs != rhs & lhs %in% factor_names & rhs %in% factor_names ~ 0.20,
      op == "~~" & lhs != rhs ~ 0.35,
      TRUE ~ as.numeric(NA)
    ),
    linetype  = dplyr::if_else(op == "~~" & lhs != rhs, 2L, 1L)
  ) %>%
  dplyr::mutate(
    stars = dplyr::case_when(pvalue < .001 ~ "***",
                             pvalue < .01  ~ "**",
                             pvalue < .05  ~ "*",
                             TRUE ~ ""),
    label = sprintf("%.2f%s", std.all, stars)
  )

layout_ids <- as.matrix(readxl::read_excel("SEM_layout.xlsx", col_names = FALSE))
layout_ids[layout_ids == ""] <- NA

# checks
in_layout <- na.omit(unique(c(layout_ids)))
stopifnot(setequal(in_layout, nodes$name))   
stopifnot(!any(duplicated(in_layout)))

g <- tidySEM::prepare_graph(
  edges = edges,
  nodes = nodes,
  layout = t(layout_ids),
  text_size = 3.6,
  rect_width = 7, rect_height = 3.5,
  ellipses_width = 6, ellipses_height = 3.5,
  variance_height = 2.5, variance_width = 2,
  arrow_angle = 15, arrow_length = .2,
  var_arrow_angle = 15, var_arrow_length = .1,
  spacing_y = 5, spacing_x = 7,
  fix_coord = FALSE
)

# colours
g$nodes$fill <- dplyr::case_when(
  g$nodes$shape == "oval"      ~ "#E8F2FF",
  g$nodes$name  == "log_age_c" ~ "#FFF2E0",
  TRUE                         ~ "#F5F5F5"
)
g$nodes$colour <- "#1F1F1F"
g$nodes$font   <- "#1F1F1F"

# edge colours
g$edges$colour <- dplyr::case_when(
  g$edges$op == "=~" ~ "#1B9E77",
  g$edges$op == "~"  ~ "#386CB0",
  g$edges$op == "~~" ~ "#6B6B6B",
  TRUE               ~ "#000000"
)
g$edges$linetype[g$edges$op == "~~"] <- 2L
g$edges$linetype[g$edges$op != "~~"] <- 1L
g$edges$size <- 0.6

# labels
g$nodes$geom_text <- TRUE
g$edges$label_fill   <- "white"
g$edges$label_colour <- "#1F1F1F"
g$edges$label_alpha  <- 1

plot(g)

```

```{r}
#| label: tab-alt-models
#| tbl-cap: "Comparison of alternative structural models including age as a predictor (robust ML, FIML)"

# single factor model 
mdl_1f <- "
g =~ matrix + mrot + hf + mg + sds + trog + vocab + math + tom
g ~ log_age_c
"

mdl_2f <- "
reasoning =~ mg + sds + hf + mrot + matrix + math
language  =~ vocab + trog + tom
reasoning ~~ language
reasoning ~ log_age_c
language  ~ log_age_c
"

mdl_3f <- "
reasoning =~ mrot + matrix + math
ef        =~ mg + sds + hf
language  =~ vocab + trog + tom
reasoning ~~ ef + language
ef ~~ language
reasoning ~ log_age_c
ef ~ log_age_c
language ~ log_age_c
"

fit_1f <- sem(mdl_1f, data = vars_core_std,
              estimator = "MLR", missing = "fiml", std.lv = TRUE, optim.method = "nlminb",
              control = list(iter.max = 50000, rel.tol = 1e-6))
fit_2f <- sem(mdl_2f, data = vars_core_std,
              estimator = "MLR", missing = "fiml", std.lv = TRUE, optim.method = "nlminb",
              control = list(iter.max = 50000, rel.tol = 1e-6))
fit_3f <- sem(mdl_3f, data = vars_core_std,
              estimator = "MLR", missing = "fiml", std.lv = TRUE, optim.method = "nlminb",
              control = list(iter.max = 50000, rel.tol = 1e-6))

want <- c("chisq", "pvalue", "tli", "cfi", "rmsea")

grab_fit <- function(fit, name){
  fm <- lavaan::fitMeasures(fit, fit.measures = want)
  tibble(
    Model  = name,
    `Chi-square` = unname(fm["chisq"]),
    `p-value`    = unname(fm["pvalue"]),
    TLI          = unname(fm["tli"]),
    CFI          = unname(fm["cfi"]),
    RMSEA        = unname(fm["rmsea"])
  )
}

fit_tbl <- bind_rows(
  grab_fit(fit_1f, "Single factor + age"),
  grab_fit(fit_2f, "Two factors + age"),
  grab_fit(fit_3f, "Three factors + age")
)

fmt_tbl <- fit_tbl %>%
  mutate(
    `Chi-square` = round(`Chi-square`, 2),
    TLI   = round(TLI, 3),
    CFI   = round(CFI, 3),
    RMSEA = round(RMSEA, 3),
    `p-value` = dplyr::case_when(
      `p-value` < .001 ~ "< .001",
      TRUE             ~ format(round(`p-value`, 3), nsmall = 3, trim = TRUE)
    )
  )

# ft <- flextable(fmt_tbl)
# ft <- autofit(ft)
# ft <- align(ft, j = c("Chi-square","p-value","TLI","CFI","RMSEA"), align = "center", part = "all")
# ft <- bold(ft, j = "Model", bold = TRUE, part = "body")
# ft <- add_header_lines(ft, values = "Model comparison with age (robust ML, FIML)")
# ft


```

```{r prep-sem-ef}
ef_tasks <- c("hf","mg","sds","mefs")

# Filter to ef tasks (from included_scores), keep first run 
ef_firstrun_scores <- included_scores |>
  filter(item_task %in% ef_tasks) |>
  arrange(user_id, item_task, run_number, time_started) |>
  group_by(user_id, item_task) |>
  slice(1) |>
  ungroup()

# one row per user
meta_ef <- ef_firstrun_scores |>
  group_by(user_id) |>
  summarise(
    age        = min(age, na.rm = TRUE),
    site       = first(na.omit(site)),
    site_label = first(na.omit(site_label)),
    dataset    = first(na.omit(dataset)),
    .groups = "drop"
  )

# Wide ef task scores (first run only)
ef_wide_scores <- ef_firstrun_scores |>
  select(user_id, item_task, metric_value) |>
  distinct() |>  # guard against accidental duplicates
  pivot_wider(
    names_from  = item_task,
    values_from = metric_value
  ) |>
  left_join(meta_ef, by = "user_id") |>
  relocate(user_id, site, site_label, dataset, age)

# keep only ef columns + metadata
vars_ef <- ef_tasks
ef_wide_scores <- ef_wide_scores |>
  select(user_id, site, site_label, dataset, age, any_of(vars_ef))

# Standardise ef indicators (same z() you used)
z <- function(x) (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
ef_wide_std <- ef_wide_scores |>
  mutate(across(all_of(vars_ef), ~ z(as.numeric(.x))))

# str(ef_wide_std[vars_ef])
```

```{r sem-ef}
#| echo: false
#| results: "hide" 

cfa_ef <-  '
EF =~ hf + mg + sds + mefs
EF ~ age
'

fit_ef <- cfa(cfa_ef, data = ef_wide_std,
              estimator = "MLR", std.lv = TRUE, missing = "fiml")
summary(fit_ef, fit.measures = TRUE, standardized = TRUE)
#print(fitMeasures(fit_ef, c("cfi","tli","rmsea","srmr", "aic", "bic")))
```

```{r}
#| label: ef-stats
#| include: false
pe_ef <- lavaan::parameterEstimates(fit_ef, standardized = TRUE)

beta_age_EF <- pe_ef |>
  dplyr::filter(lhs == "EF", op == "~", rhs == "age") |>
  dplyr::pull(std.all) |> as.numeric()

r2_EF <- as.numeric(lavaan::lavInspect(fit_ef, "r2")["EF"])

fmt <- function(x, d = 2) formatC(x, digits = d, format = "f")
# standardised loadings (λ) for EF =~ hf + mg + sds + mefs
load_tab <- pe_ef |>
  dplyr::filter(op == "=~", lhs == "EF", rhs %in% c("hf","mg","sds","mefs")) |>
  dplyr::select(rhs, std.all) |>
  tibble::deframe()  # named vector: c(hf = ., mg = ., sds = ., mefs = .)

```

\clearpage

### External validity

To assess external validity for the executive function and language tasks, we fitted a single-factor Executive Function model with Hearts and Flowers, Memory, Same and Different, and the external measure, the Minnesota Executive Function Scale (MEFS) as indicators (@fig-semplot-ef). We modeled age as a predictor of the latent Executive Function factor. Model fit was good: CFI: `r round(lavaan::fitMeasures(fit_ef)["cfi"], 3)`, RMSEA: `r round(lavaan::fitMeasures(fit_ef)["rmsea"], 3)`, RMSEA p-value: `r pval_fmt(lavaan::fitMeasures(fit_ef)["rmsea.pvalue"])`. Standardized loadings were: Hearts and Flowers = `r fmt(load_tab["hf"])`, Memory = `r fmt(load_tab["mg"])`, Same and Different = `r fmt(load_tab["sds"])`, MEFS = `r fmt(load_tab["mefs"])`, indicating that MEFS aligns closely with the latent Executive Function construct and providing good evidence for external validity. Age strongly predicted Executive Function ($\beta$ = `r round(beta_age_EF, 2)`; $R^2$ = `r round(r2_EF, 2)`), consistent with expected developmental gains.

```{r}
#| label: fig-semplot-ef
#| fig-cap: "SEM for Executive Function model."
#| fig-height: 4

nodes <- get_nodes(fit_ef, columns = c("name","shape")) |>
  dplyr::left_join(tibble::tibble(
    name   = c("age","EF","hf","mg","sds","mefs"),
    pretty = c("Age (years)","Executive Function",
               "Hearts and Flowers","Memory","Same and Different","MEFS")
  ), by = "name") |>
  dplyr::mutate(label = dplyr::coalesce(pretty, name))

# edges from standardized parameter estimates ---
pe_ef <- lavaan::parameterEstimates(fit_ef, standardized = TRUE)

edges <- pe_ef |>
  dplyr::filter(op %in% c("~","=~","~~")) |>
  dplyr::transmute(
    lhs, rhs, op,
    from = dplyr::if_else(op == "~", rhs, lhs),
    to   = dplyr::if_else(op == "~", lhs, rhs),
    std.all, pvalue,
    arrow = dplyr::case_when(
      op == "~~" & lhs == rhs ~ "both",  # variance loop
      op == "~~" & lhs != rhs ~ "none",  # covariance
      TRUE ~ "last"                       # regressions/loadings
    ),
    curvature = dplyr::if_else(op == "~~" & lhs != rhs, 50, NA_real_),
    linetype  = dplyr::if_else(op == "~~" & lhs != rhs, 2, 1),
    label = dplyr::case_when(
      op %in% c("~","=~") ~ sprintf("%.2f%s", std.all,
        dplyr::case_when(pvalue < .001 ~ "***",
                         pvalue < .01  ~ "**",
                         pvalue < .05  ~ "*", TRUE ~ "")),
      TRUE ~ sprintf("%.2f", std.all)
    )
  ) 

# top-down layout
layout_ef <- matrix(
  c(
    NA, NA, "age", NA, NA,
    NA, NA, "EF",  NA, NA,
    "hf","mg", NA, "sds","mefs" 
  ),
  nrow = 3, byrow = TRUE
)

g_ef <- tidySEM::prepare_graph(
  edges = edges,
  nodes = nodes,
  layout = layout_ef,        
  text_size = 2.8,
  rect_width = 3.4, rect_height = 1.5,
  ellipses_width = 5, ellipses_height = 2,
  variance_height = 3, variance_width = 1.5,
  arrow_angle = 15, arrow_length = .10,
  var_arrow_angle = 15, var_arrow_length = .10,
  spacing_y = 6, spacing_x = 5,
  fix_coord = TRUE
)

g_ef$edges$label_fill   <- "white"
g_ef$edges$label_colour <- "black"
g_ef$edges$label_size   <- 2.8

plot(g_ef)
```

```{r prep-sem-lang}
lang_vars <- c("swr","sre","pa","vocab","trog")

# first run per user–task
lang_firstrun_scores <- included_scores |>
  filter(item_task %in% lang_vars) |>
  arrange(user_id, item_task, run_number, time_started) |>
  group_by(user_id, item_task) |>
  slice(1) |>
  ungroup()

meta_lang <- lang_firstrun_scores |>
  group_by(user_id) |>
  summarise(
    age        = min(age, na.rm = TRUE),
    site       = first(na.omit(site)),
    site_label = first(na.omit(site_label)),
    dataset    = first(na.omit(dataset)),
    .groups = "drop"
  )

# create wide df 
lang_wide_scores <- lang_firstrun_scores |>
  select(user_id, item_task, metric_value) |>
  distinct() |>
  pivot_wider(
    names_from  = item_task,
    values_from = metric_value
  ) |>
  left_join(meta_lang, by = "user_id") |>
  relocate(user_id, site, site_label, dataset, age)

# keep only language columns + metadata
lang_wide_scores <- lang_wide_scores |>
  select(user_id, site, site_label, dataset, age, any_of(lang_vars))

# standardise 
z <- function(x) (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
lang_wide_std <- lang_wide_scores |>
  mutate(across(all_of(lang_vars), \(x) z(as.numeric(x))))
# mutate(across(all_of(intersect(lang_vars, names(.))), ~ z(as.numeric(.x))))
```

```{r sem-lang}
#| echo: false
#| results: "hide" 

# model
cfa_lang <-  '
Language =~ swr + sre + pa + trog + vocab
Language ~ age
'

fit_lang <- cfa(cfa_lang, data = lang_wide_std,
                estimator = "MLR", std.lv = TRUE, missing = "fiml")
summary(fit_lang, fit.measures = TRUE, standardized = TRUE)
pe_lang <- lavaan::parameterEstimates(fit_lang, standardized = TRUE)
fm_lang <- lavaan::fitMeasures(fit_lang)

# standardised beta and R^2
beta_age_Lang <- pe_lang |>
dplyr::filter(lhs == "Language", op == "~", rhs == "age") |>
dplyr::pull(std.all) |> as.numeric()
r2_Lang <- as.numeric(lavaan::lavInspect(fit_lang, "r2")["Language"])

# standarised loadings
load_tab_lang <- pe_lang |>
dplyr::filter(op == "=~", lhs == "Language",
rhs %in% c("swr","sre","pa","trog","vocab")) |>
dplyr::select(rhs, std.all) |>
tibble::deframe()

```

\clearpage

We ran a similar model to determine the external validity of the language and literacy measures (Sentence Understanding and Vocabulary) against ROAR tasks, including measures of Phonological Awareness (ROAR-Phoneme), Single Word Reading (ROAR-Word) and Sentence Reading Efficiency (ROAR-Sentence). We fitted a single latent Language factor with ROAR tasks plus Sentence Understanding and Vocabulary as indicators, and modeled age as a predictor of the latent factor (@fig-semplot-lang). Model fit was again good, CFI: `r round(fm_lang["cfi"], 3)`, RMSEA: `r round(fm_lang["rmsea"], 3)`, RMSEA p-value: 
`r pval_fmt(lavaan::fitMeasures(fit_lang)["rmsea.pvalue"])`. Standardized loadings were: ROAR-Word = `r round(load_tab_lang["swr"], 2)`, ROAR-Sentence = `r round(load_tab_lang["sre"], 2)`, ROAR-Phoneme = `r round(load_tab_lang["pa"], 2)`, Sentence Understanding = `r round(load_tab_lang["trog"], 2)`, Vocabulary = `r round(load_tab_lang["vocab"], 2)`. Age predicted Language ($\beta$ = `r round(beta_age_Lang, 2)`; $R^2$ = `r round(r2_Lang, 2)`) consistent with expected developmental gains. 

```{r}
#| label: fig-semplot-lang
#| fig-cap: "SEM for Language model."
#| fig-height: 4

nodes_lang <- get_nodes(fit_lang, columns = c("name","shape")) |>
  dplyr::left_join(tibble::tibble(
    name   = c("age","Language","trog","vocab","swr","sre","pa"),
    pretty = c("Age (years)","Language",
               "Sentence Understanding","Vocabulary",
               "ROAR-Word","ROAR-Sentence",
               "ROAR-Phoneme")
  ), by = "name") |>
  dplyr::mutate(label = dplyr::coalesce(pretty, name))

# edges (standardized, with stars on loadings/paths)
pe_lang <- lavaan::parameterEstimates(fit_lang, standardized = TRUE)

edges_lang <- pe_lang |>
  dplyr::filter(op %in% c("~","=~","~~")) |>
  dplyr::transmute(
    lhs, rhs, op,
    from = dplyr::if_else(op == "~", rhs, lhs),
    to   = dplyr::if_else(op == "~", lhs, rhs),
    std.all, pvalue,
    arrow = dplyr::case_when(
      op == "~~" & lhs == rhs ~ "both",     
      op == "~~" & lhs != rhs ~ "none",     
      TRUE ~ "last"                         
    ),
    curvature = dplyr::if_else(op == "~~" & lhs != rhs, 50, NA_real_),
    linetype  = dplyr::if_else(op == "~~" & lhs != rhs, 2L, 1L),
    label = dplyr::case_when(
      op %in% c("~","=~") ~ sprintf("%.2f%s", std.all,
        dplyr::case_when(pvalue < .001 ~ "***",
                         pvalue < .01  ~ "**",
                         pvalue < .05  ~ "*", TRUE ~ "")),
      TRUE ~ sprintf("%.2f", std.all)
    )
  ) 

# top-down layout
layout_lang <- matrix(
  c(
    NA, NA, "age", NA, NA, NA,
    NA, NA, NA, NA, NA, NA,
    NA, NA, "Language", NA, NA, NA, 
    NA, NA, NA, NA, NA, NA,
    "trog","vocab", NA, NA, NA,
    NA, NA, NA, "swr","sre","pa"         
  ),
  nrow = 6, byrow = TRUE
)

g_lang <- tidySEM::prepare_graph(
  edges = edges_lang,
  nodes = nodes_lang,
  layout = layout_lang,        
  text_size = 2.8,
  rect_width = 3.6, rect_height = 3,
  ellipses_width = 5, ellipses_height = 4,
  spacing_x = 5,
  spacing_y = 6,
  variance_height = 3, variance_width = 2,
  arrow_angle = 15, arrow_length = .10,
  fix_coord = FALSE
)

g_lang$edges$label_fill   <- "white"
g_lang$edges$label_colour <- "black"
g_lang$edges$label_size   <- 3

plot(g_lang)
```

```{r}
#| label: fig-age-trends-cat
#| fig-cap: "Age related trends by task for CAT only children."
#| fig-width: 8
#| fig-height: 8

cat_only <- coretask_scores %>%
  filter(adaptive == TRUE, site_label != "uniandes-co") %>% # drop Columbia
  droplevels()

ggplot(cat_only, aes(age, metric_value)) +
  ggh4x::facet_nested_wrap(vars(task_category, task),
                           nest_line = element_line(), solo_line = TRUE,
                           axes = "x", scales = "free_y") +
  geom_smooth(aes(colour = site_label, group = site_label),
              method = "gam", formula = y ~ s(x, bs = "re"), se = FALSE) +
  geom_point(aes(colour = site_label), alpha = 0.3, size = 1) +
  scale_x_continuous(breaks = seq(6, 14, 2)) +
  .scale_colour_site() +
  guides(colour = guide_legend(override.aes = list(shape = NA, linetype = 1, linewidth = 2))) +
  labs(x = "Age (years)", y = "Ability (IRT score)", colour = "Site",
       title = "Ability vs Age (CAT only)") +
  theme(legend.position = "bottom")

```

\clearpage

# Adaptive Task Construction

We have already adapted and piloted several of the LEVANTE core tasks as CATs, primarily in the mpieva-de and western_ca pilot sites. To date, these include Sentence Understanding, Vocabulary, Shape Rotation, Pattern Matching, Same and Different, and Math. These tasks maintain an ability score, $\theta$, as an estimate of the participant’s skill level that is updated at the end of each trial. This updated theta is then used to select an item best suited to their estimated ability, which both improves test-taker experience and yields more information on participant skill per item, allowing for a shorter task with fewer items. See @fig-age-trends-cat for age-related ability trends in the tasks currently adapted to the CAT format. These trends mirror those observed in the full dataset (see @fig-age-trends), with similar site-level patterns and positive developmental slopes across domains.

The CAT tasks use an adaptive algorithm made available by the open source jsCat JavaScript library [@ma2025]. The present LEVANTE CAT implementation varies difficulty and guessing for each item while holding both discrimination and upper asymptote constant at 1. Items are selected based on Maximum Fisher Information, and theta is updated according to a maximum likelihood estimator, with limits of -6 and 6. 

The LEVANTE CATs are configurable with respect to the initial value of $\theta$ and the conditions for ending the task. The starting $\theta$ is set at 0 for all CAT tasks currently in use, but can be lowered or raised according to the researcher’s prior expectation of participant ability, for example according to age. Current CAT implementations use time-based stopping rules to facilitate group testing so that all children end at approximately the same time. Sentence Understanding, Shape Rotation, and Vocabulary each have time limits currently set to 4 minutes. Pattern Matching was set to 6 minutes to allow for the increased time typically required to complete items in this task. Items in these tasks are presented together in a single block. 

Same and Different and Math are each divided into three blocks presented sequentially, with per-block stopping based on number of items. These CATs select from the list of items specific to their current block and proceed to the next block once the target number of items is reached, maintaining one overarching ability estimate for the entire task. Same and Different and Math have time limits of 6 minutes and 8 minutes, respectively. 

# General Discussion

In this paper, we presented the LEVANTE core tasks -- nine psychometrically grounded tasks for children aged 5–12 that provide efficient, reliable, and valid assessment across language, mathematics, reasoning, executive function, and social cognition. LEVANTE is designed to tackle three core challenges: to span development and enable valid comparisons between younger and older children; to ensure cross-cultural comparability so that measures function equivalently across diverse contexts; and to make the tasks openly available (non-commercial) to maximize access, transparency, and reproducibility.

Using pilot data from three different countries, collected via three different administration methods (in-school, in-lab, and at-home administration), we provided preliminary evidence of reliability and validity. Further, using computer adaptive testing algorithms, we are able to deploy efficient versions of these tasks that produce scores for children across a wide range of abilities in just a few minutes. While these tasks do not all yet meet conventional thresholds for reliability (i.e., CFI ≥ 0.95, TLI ≥ 0.95, RMSEA ≤ 0.05), it is important to note that they still fill a gap: many tasks used with children are not assessed for reliability at all and may well be substantially less reliable than those presented here. 

Several of the nine tasks remain under construction, and we expect gains in reliability and validity as items are refined, distractors improved, and instructions localized. We are actively monitoring task performance and iteratively refining items as new data accrue across sites. This includes updating IRT calibrations, conducting routine checks for differential item functioning, and making revisions guided by fit diagnostics and error patterns. We are attempting to strike a balance between minimizing bias and ensuring that observed differences reflect genuine developmental variation rather than artifacts of the measurement process.

We hope that the core tasks presented here will become the backbone of the LEVANTE framework, in which sites around the world collaborate to collect longitudinal data measuring children's learning and development as well as the overlapping contexts in which they develop. De-identified data collected using the tasks presented here flow into a shared repository, becoming part of a large-scale dataset that we hope will enable a wide variety of downstream investigations. The power of these data is that -- especially when combined with contextual data about children’s caregivers, home environment, school, and community environment -- they afford one analysis of the sources of developmental and contextual variability at global scale.

Towards this goal, one strength of our approach here is that it easily accommodates future internationalization and adaptation. In fact, at the time of writing we are working on adaptations for two more languages (French and Dutch) and intend to further broaden the set of languages for which the tasks are available. As part of this process, we have created a website using CrowdIn for generating and reviewing translation materials. The LEVANTE researcher website documents the use of this platform and our ongoing language adaptation efforts: <https://researcher.levante-network.org>. 

Our current task set is designed for children ages 5--12, but we are currently pursuing downward extension of many of these tasks for children ages 2--4. By design, the forced choice, tablet-based format of our tasks makes them highly amenable to use with younger children [@frank2016]. In our downward extension work, we are adding easier items to each task, increasing the number of practice trials, and increasing the amount of instructions. We anticipate that not all tasks will be usable for all ages, but for at least the executive function and language tasks there is strong evidence of usability with children under age 5 [@davidson2006;@bishop1982;@long_ma_tan_silverman_frank_yeatman_2025]. 

In sum, we have taken steps here towards creating a core set of tasks for measuring children’s learning and development. We hope that this work is the beginning, rather than the end, of a process of iterative refinement and growth that results in a common, open measurement toolkit for developmental science.

\newpage

# References

::: {#refs}
:::

# Appendix

# Age Slopes by Task {#apx-em-trends}

# Predicted Ability v Age {#apx-predicted-age-slopes}




