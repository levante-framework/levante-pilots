```{r setup}
library(tidyverse)
library(here)
library(glue)
library(ggforce)
library(ggthemes)
library(ggh4x)

source(here("03_summaries", "plotting_helper.R"))
threshold_n <- 10
```

First downward extension analysis. There's lots more to do here. 

* We don't have any SDS data because we don't have it scored under the IRT model yet. 
* We're missing a bit of memory data because the scoring excludes kids that don't get *any* trials right. 

```{r}
sites <- c("us_pilot")

participants <- sites |>
  set_names() |>
  map(\(s) read_rds(here(glue("00_prepped_data/{s}/participants.rds")))) |>
  list_rbind(names_to = "site")
```

Combine all task scores into one plot!

```{r}
score_files <- list.files(here("02_scored_data/scores"), pattern = "*.rds",
                          full.names = TRUE) 
score_list <- score_files |> map(read_rds)
# exclude_tasks <- c("hostile-attribution", "pa-es")
# score_list <- read_rds("scores/combined_scores.rds")

run_ages <- participants |>
  select(user_id, ages) |>
  unnest(ages)

scores <- score_list |>
  bind_rows() |>
  rename(task = task_id) |>
  left_join(run_ages) |>
  filter(!is.na(age)) |>
  left_join(task_categories) |>
  group_by(task) |>
  mutate(task_label = glue("{task}\n(n = {n_distinct(user_id)})")) |>
  ungroup() 
  
task_categories_vec <- levels(scores$task_category)
task_pal <- ptol_pal()(length(task_categories_vec)) |> set_names(task_categories_vec)
```

```{r}
task_scores <- scores |>
  mutate(metric_type = if_else(str_detect(metric_type, "ability"), "ability", metric_type)) |>
  filter(!is.na(metric_value)) |>
  inner_join(task_metrics) 

task_plot_pooled(task_scores, ylab = "Score", nr = 1, y_axis = c(3,4,5))
ggsave(here("03_summaries/plots/downward_extension.png"), width = 8, height = 4)

```
```

